{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVUPI5m_jcvF",
    "outputId": "862dcf47-a366-4260-8384-8ffce96ff40f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 1 - Autoencoders\n",
    "## Deep Learning\n",
    "\n",
    "### Carlos Emiliano Rodríguez Núñez\n",
    "\n",
    "#### CelebA VAE\n",
    "* Objetivos:\n",
    "Comprender los principios fundamentales de los autoencoders y su aplicación en deep learning generativo.\n",
    "Implementar un autoencoder básico y variacionales para una tarea específica, como reducción de dimensión, denoising o generación de imágenes.\n",
    "Analizar el rendimiento y las características de las representaciones aprendidas por los autoencoders.\n",
    " \n",
    "\n",
    "* Descripción\n",
    "Deberán seleccionar un conjunto de datos adecuado para su proyecto, que puede ser de imágenes, texto o cualquier otro tipo que permita la aplicación de autoencoders.\n",
    "Implementar un autoencoder, como un variacional (VAE) o un autoencoder convolucional, dependiendo de la naturaleza del conjunto de datos y el objetivo del proyecto.\n",
    "El proyecto incluirá una fase de experimentación donde los deberán entrenar, ajustar y evaluar sus modelos.\n",
    "Presentar sus resultados a través de un informe escrito y una presentación, discutir la implementación, los desafíos encontrados, el rendimiento de sus modelos y las aplicaciones potenciales de su trabajo.\n",
    " \n",
    "\n",
    "* Rúbrica del Proyecto\n",
    "La rúbrica está dividida en varias categorías, cada una con su propio conjunto de criterios y una escala de puntuación. La puntuación máxima posible es de 100 puntos.\n",
    "\n",
    "1. Documentación y Presentación (20 puntos)\n",
    "2. Diseño e Implementación (40 puntos)\n",
    "3. Experimentación y Análisis (30 puntos)\n",
    "4. Innovación y Creatividad (10 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BjjehA4DdNb5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import binary_crossentropy, mean_squared_error\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos librerías y determinamos el tamaño del espacio latente, del batch y las épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "G1K-MY7weXzN"
   },
   "outputs": [],
   "source": [
    "batch_size = 100 # hacemos de 100 en 100 \n",
    "latent_dim = 64 # dimensión espacio latente\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se utiliza google colab, se elimina lo que se tenga en la carpeta content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3ul6A11qiBH6"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/extracted_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos un dataset llamado CelebA de pytorch, el cuál contiene miles de imágenes de celebridades de todos los tiempos. Estas imágenes no están centradas ni orientadas, por lo que puede dificultar un poco el entrenamiento de un modelo `VAutoEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1czuIbpp2UY1",
    "outputId": "1e0c3cf1-851b-4c81-9167-3d3553c5336d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta __MACOSX eliminada con éxito.\n",
      "¡Archivo ZIP descomprimido y carpeta __MACOSX eliminada con éxito!\n"
     ]
    }
   ],
   "source": [
    "# Ruta al archivo ZIP que deseas descomprimir\n",
    "zip_file_path = \"/content/drive/MyDrive/ITESO/APRENDIZAJE_PROFUNDO/proyecto_VAE/celebA.zip\" # \"/content/Archive.zip\" \"   /content/Archive_2.zip\"\n",
    "\n",
    "# Directorio de destino donde se extraerán los archivos\n",
    "extract_to_directory = \"/content/extracted_files\"\n",
    "\n",
    "# Crear el directorio de destino si no existe\n",
    "os.makedirs(extract_to_directory, exist_ok=True)\n",
    "\n",
    "# Descomprimir el archivo ZIP\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_directory)\n",
    "\n",
    "# Eliminar la carpeta __MACOSX\n",
    "macosx_folder = os.path.join(extract_to_directory, '__MACOSX')\n",
    "if os.path.exists(macosx_folder):\n",
    "    shutil.rmtree(macosx_folder)\n",
    "    print(\"Carpeta __MACOSX eliminada con éxito.\")\n",
    "\n",
    "print(\"¡Archivo ZIP descomprimido y carpeta __MACOSX eliminada con éxito!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función para cargar las imágenes en pixeles de `32x32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgHHxAyS2HJe",
    "outputId": "a642b53e-c56f-4134-a224-1d38187c036a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de x_train: (23967, 32, 32, 3)\n",
      "Dimensiones de y_train: (23967,)\n",
      "Dimensiones de x_test: (6033, 32, 32, 3)\n",
      "Dimensiones de y_test: (6033,)\n"
     ]
    }
   ],
   "source": [
    "# Función para cargar las imágenes\n",
    "def load_images_from_folder(folder, target_size=(32, 32)): #------------------------------------------------------------------------------- size en vez de 32\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            # Redimensionar la imagen\n",
    "            img = img.resize(target_size)\n",
    "            # Convertir a formato RGB y asegurarse de que tenga solo 3 canales\n",
    "            img = img.convert('RGB')\n",
    "            images.append(np.array(img))\n",
    "            labels.append(0 if random.random() < 0.8 else 1)\n",
    "    return images, labels\n",
    "\n",
    "# Directorio donde están tus imágenes\n",
    "root_directory = '/content/extracted_files/celebA' \n",
    "\n",
    "# Cargar las imágenes y las etiquetas\n",
    "images, labels = load_images_from_folder(root_directory)\n",
    "\n",
    "# Dividir las imágenes y las etiquetas en conjuntos de entrenamiento y prueba\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for img, label in zip(images, labels):\n",
    "    if label == 0:\n",
    "        x_train.append(img)\n",
    "        y_train.append(label)\n",
    "    else:\n",
    "        x_test.append(img)\n",
    "        y_test.append(label)\n",
    "\n",
    "# Convertir listas a numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Imprimir las dimensiones de los conjuntos de entrenamiento y prueba\n",
    "print('Dimensiones de x_train:', x_train.shape)\n",
    "print('Dimensiones de y_train:', y_train.shape)\n",
    "print('Dimensiones de x_test:', x_test.shape)\n",
    "print('Dimensiones de y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que se tienen aproximadamente `+ 30 mil imágenes`, de las cuales se utilizarán 24 mil para el entrenamiento.\n",
    "Los AE tienen una entrada `x -> x` y salida x, pero es una práctica interesante separar en train y test. No se utilizará la parte 'y_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "AEjhGDhm32cO",
    "outputId": "80e9d972-c501-4da9-a019-6f11752bdbfb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convertir el lote de imágenes a un arreglo NumPy\n",
    "images = x_train\n",
    "\n",
    "# Seleccionar una imagen aleatoria del lote\n",
    "index = np.random.randint(len(images))\n",
    "\n",
    "image = images[0]\n",
    "# Mostrar la imagen\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos una imágen aleatoria de 32x32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VR1H1ePFd12X",
    "outputId": "7586fef3-232f-4490-d290-00a489f2f73e"
   },
   "source": [
    "Realizamos un escalamiento porque los valores van hasta el 255 y para el AutoEncoder se requiere que esté entre `0 y 1` porque son probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kY7DYiQHelc8"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos la función de sampling que utiliza el `promedio` y la `varianza` para generar `el espacio latente`. Creamos el modelo de CONV2D desde 32 hasta 128 con funciones de activación tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7nl_nIdtfW3Q"
   },
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "  z_mean, z_log_var = args\n",
    "\n",
    "  dim = K.int_shape(z_mean)[1]\n",
    "\n",
    "  epsilon = K.random_normal(shape = (K.shape(z_mean)[0], latent_dim))\n",
    "\n",
    "  return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "  # build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Conv2D(32, 3, activation = \"tanh\", strides = 2, padding = \"same\")(inputs)\n",
    "x = Conv2D(64, 3, activation = \"tanh\", strides = 2, padding = \"same\")(x)\n",
    "#-----------------------------------------------------------------------\n",
    "x = Conv2D(128, 3, activation = \"tanh\", strides = 2, padding = \"same\")(x)\n",
    "# x = Conv2D(256, 3, activation = \"relu\", strides = 2, padding = \"same\")(x)\n",
    "\n",
    "\n",
    "# reshape flatten, hay q guardar las salidas de la capa\n",
    "shape_before_flat = K.int_shape(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation = \"tanh\")(x) # flat para encontrar la media y la varianca\n",
    "\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Scp-QhcKj0N8",
    "outputId": "1152f5ff-9924-482a-c671-ccd1450aa9da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)  [(None, 32, 32, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 16, 16, 32)           896       ['encoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)            73856     ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 2048)                 0         ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  524544    ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " z_mean (Dense)              (None, 64)                   16448     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)           (None, 64)                   16448     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " z (Lambda)                  (None, 64)                   0         ['z_mean[0][0]',              \n",
      "                                                                     'z_log_var[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 650688 (2.48 MB)\n",
      "Trainable params: 650688 (2.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzxRizb0-r_e",
    "outputId": "511b1837-82a0-446a-93e1-8bf143e00b82"
   },
   "source": [
    "Realizamos el decoder con un \"efecto espejo\" al encoding, solo que poniendo una capa final con función de activación `sigmoide` que de probabilidades de valores entre `0 - 1`, que son las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pwcQWX4kACQ",
    "outputId": "7d279e66-28f9-4909-bf89-20e1eddcd3a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              133120    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 8, 8, 128)         147584    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 16, 16, 64)        73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 32, 32, 32)        18464     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2D  (None, 32, 32, 3)         867       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 373827 (1.43 MB)\n",
      "Trainable params: 373827 (1.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "\n",
    "# Check if work\n",
    "x = Dense(np.prod(shape_before_flat[1:]), activation = \"tanh\")(latent_inputs)\n",
    "x = Reshape(shape_before_flat[1:])(x)\n",
    "#------------------------------------------------------------------------------------------------\n",
    "# x = Conv2DTranspose(256, 3, activation = \"relu\", strides = 2, padding = \"same\")(x) tanh relu\n",
    "x = Conv2DTranspose(128, 3, activation = \"tanh\", strides = 2, padding = \"same\")(x)\n",
    "#------------------------------------------------------------------------------------------------\n",
    "x = Conv2DTranspose(64, 3, activation = \"tanh\", strides = 2, padding = \"same\")(x)\n",
    "x = Conv2DTranspose(32, 3, activation = \"tanh\", strides = 2, padding = \"same\")(x)\n",
    "\n",
    "outputs = Conv2DTranspose(3, 3, activation = \"sigmoid\", padding = \"same\")(x) # sigmoid\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8WexQpynKxX",
    "outputId": "b8a4f1b3-e6a4-4642-b365-745affaa9881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)  [(None, 32, 32, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        [(None, 64),                 650688    ['encoder_input[0][0]']       \n",
      "                              (None, 64),                                                         \n",
      "                              (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " decoder (Functional)        (None, 32, 32, 3)            373827    ['encoder[0][2]']             \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 16, 16, 32)           896       ['encoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 64)             18496     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 128)            73856     ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 2048)                 0         ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  524544    ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " z_log_var (Dense)           (None, 64)                   16448     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " z_mean (Dense)              (None, 64)                   16448     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 64)                   0         ['z_log_var[0][0]']           \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.square (TFOpLambda  (None, 64)                   0         ['z_mean[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.reshape_1 (TFOpLambda)   (None,)                      0         ['decoder[0][0]']             \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)     (None,)                      0         ['encoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLamb  (None, 64)                   0         ['tf.__operators__.add[0][0]',\n",
      " da)                                                                 'tf.math.square[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.exp (TFOpLambda)    (None, 64)                   0         ['z_log_var[0][0]']           \n",
      "                                                                                                  \n",
      " tf.convert_to_tensor (TFOp  (None,)                      0         ['tf.reshape_1[0][0]']        \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)        (None,)                      0         ['tf.reshape[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.subtract_1 (TFOpLa  (None, 64)                   0         ['tf.math.subtract[0][0]',    \n",
      " mbda)                                                               'tf.math.exp[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.squared_difference  (None,)                      0         ['tf.convert_to_tensor[0][0]',\n",
      "  (TFOpLambda)                                                       'tf.cast[0][0]']             \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum (TFOpLa  (None,)                      0         ['tf.math.subtract_1[0][0]']  \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpL  ()                           0         ['tf.math.squared_difference[0\n",
      " ambda)                                                             ][0]']                        \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None,)                      0         ['tf.math.reduce_sum[0][0]']  \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None,)                      0         ['tf.math.reduce_mean[0][0]', \n",
      " OpLambda)                                                           'tf.math.multiply[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFO  ()                           0         ['tf.__operators__.add_1[0][0]\n",
      " pLambda)                                                           ']                            \n",
      "                                                                                                  \n",
      " add_loss (AddLoss)          ()                           0         ['tf.math.reduce_mean_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1024515 (3.91 MB)\n",
      "Trainable params: 1024515 (3.91 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name = 'vae')\n",
    "\n",
    "\n",
    "# reconstruction_loss = mean_squared_error(K.flatten(inputs), K.flatten(outputs) * input_shape[0] * input_shape[1])\n",
    "reconstruction_loss = mean_squared_error(K.flatten(inputs), K.flatten(outputs))\n",
    "\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68lJqKp4n7M3",
    "outputId": "735a5746-458d-4a12-d08e-398ddec6718e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0776 - val_loss: 0.0766\n",
      "Epoch 2/50\n",
      "240/240 [==============================] - 3s 12ms/step - loss: 0.0773 - val_loss: 0.0764\n",
      "Epoch 3/50\n",
      "240/240 [==============================] - 3s 12ms/step - loss: 0.0771 - val_loss: 0.0763\n",
      "Epoch 4/50\n",
      "240/240 [==============================] - 3s 14ms/step - loss: 0.0770 - val_loss: 0.0762\n",
      "Epoch 5/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0769 - val_loss: 0.0761\n",
      "Epoch 6/50\n",
      "240/240 [==============================] - 3s 12ms/step - loss: 0.0768 - val_loss: 0.0761\n",
      "Epoch 7/50\n",
      "240/240 [==============================] - 3s 12ms/step - loss: 0.0768 - val_loss: 0.0760\n",
      "Epoch 8/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0767 - val_loss: 0.0759\n",
      "Epoch 9/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0767 - val_loss: 0.0759\n",
      "Epoch 10/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0767 - val_loss: 0.0759\n",
      "Epoch 11/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0766 - val_loss: 0.0759\n",
      "Epoch 12/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0766 - val_loss: 0.0759\n",
      "Epoch 13/50\n",
      "240/240 [==============================] - 3s 14ms/step - loss: 0.0766 - val_loss: 0.0758\n",
      "Epoch 14/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0766 - val_loss: 0.0758\n",
      "Epoch 15/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0766 - val_loss: 0.0758\n",
      "Epoch 16/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0766 - val_loss: 0.0758\n",
      "Epoch 17/50\n",
      "240/240 [==============================] - 3s 15ms/step - loss: 0.0765 - val_loss: 0.0758\n",
      "Epoch 18/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0766 - val_loss: 0.0757\n",
      "Epoch 19/50\n",
      "240/240 [==============================] - 4s 15ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 20/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 21/50\n",
      "240/240 [==============================] - 3s 15ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 22/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 23/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 24/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 25/50\n",
      "240/240 [==============================] - 3s 14ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 26/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 27/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 28/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 29/50\n",
      "240/240 [==============================] - 4s 15ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 30/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 31/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 32/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 33/50\n",
      "240/240 [==============================] - 3s 14ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 34/50\n",
      "240/240 [==============================] - 3s 14ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 35/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 36/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 37/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 38/50\n",
      "240/240 [==============================] - 3s 14ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 39/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 40/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 41/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 42/50\n",
      "240/240 [==============================] - 3s 14ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 43/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 44/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 45/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0758\n",
      "Epoch 46/50\n",
      "240/240 [==============================] - 3s 14ms/step - loss: 1.6230 - val_loss: 0.0757\n",
      "Epoch 47/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 48/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 49/50\n",
      "240/240 [==============================] - 3s 13ms/step - loss: 0.0765 - val_loss: 0.0757\n",
      "Epoch 50/50\n",
      "240/240 [==============================] - 3s 14ms/step - loss: 0.0765 - val_loss: 0.0757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x78dee01098d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the autoencoder\n",
    "vae.fit(x_train,\n",
    "        epochs=epochs, # epochs\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el siguiente código y usando el x_test, realizamos un encoding para obtener la media y varianza, sacar el espacio latente y después decodeamos para generár la imágen de una celebridad totalmente inventada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ISXQtvy8Es8A",
    "outputId": "e07520c1-a58a-4406-a509-8d8037c355ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - 0s 2ms/step\n",
      "189/189 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Obtener z_mean, z_log_var para un lote de imágenes de entrada\n",
    "z_mean_batch, z_log_var_batch, _ = encoder.predict(x_test)\n",
    "\n",
    "# Utilizar la función de muestreo para generar muestras de la distribución latente\n",
    "latent_samples = sampling([z_mean_batch, z_log_var_batch])\n",
    "\n",
    "# Decodificar las muestras generadas para obtener imágenes \"inventadas\"\n",
    "decoded_images = decoder.predict(latent_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "8ubsQe5zE61D",
    "outputId": "bbc0700a-9233-4fa0-f02c-a0da6c41d369"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Elegir una de las imágenes generadas para mostrar\n",
    "image_to_show = decoded_images[0]\n",
    "\n",
    "# Mostrar la imagen generada\n",
    "plt.imshow(image_to_show)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nmms se ve bien turbio."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
